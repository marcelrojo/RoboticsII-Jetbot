{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9a9cdc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import tf2onnx\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    GlobalAveragePooling2D,\n",
    "    Conv2DTranspose,\n",
    "    concatenate,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence, plot_model\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import (\n",
    "    load_img,\n",
    "    img_to_array,\n",
    "    ImageDataGenerator,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Generator\n",
    "\n",
    "import random\n",
    "\n",
    "random_state = 123456\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "55f8572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_data(data_frame, window_size=10):\n",
    "\n",
    "    window = 2 * window_size + 1  \n",
    "    return data_frame.rolling(window=window, min_periods=1, center=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37edca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_paths) -> Generator[np.array, float, float]:\n",
    "    for csv_path in csv_paths:\n",
    "        csv_data = pd.read_csv(csv_path, header=None)\n",
    "        csv_data.columns = [\"id\", \"forward\", \"left\"]\n",
    "        \n",
    "        csv_data = smooth_data(csv_data)\n",
    "        \n",
    "        image_paths = glob(f\"{csv_path.removesuffix('.csv')}/*.jpg\")\n",
    "        random.shuffle(image_paths)\n",
    "        \n",
    "        for image_path in image_paths:\n",
    "            image_data = load_img(image_path)\n",
    "            image_number = int(Path(image_path).name.removesuffix(\".jpg\"))\n",
    "            row = csv_data[csv_data[\"id\"]==image_number]\n",
    "            if row.empty:\n",
    "                continue\n",
    "            yield image_data, row[\"forward\"].values[0], row[\"left\"].values[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "76b65284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_preprocess(paths, image_size, batch_size):\n",
    "    def preprocess(image, image_size):\n",
    "        image = img_to_array(image).astype(np.uint8)\n",
    "        \n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        \n",
    "        img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        img_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(img_gray)\n",
    "        img_blurred = cv2.GaussianBlur(img_clahe, (3, 3), 0)\n",
    "        \n",
    "        img_crop = img_blurred[image_size//2:image_size, 0:image_size]\n",
    "        img_crop = cv2.resize(img_crop, (image_size, image_size))\n",
    "        \n",
    "        pil_image = Image.fromarray(img_crop.astype(np.uint8), \"L\")\n",
    "        \n",
    "        return pil_image\n",
    "    \n",
    "    def augment(image, forward, left):\n",
    "        image_flip = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        left_flipped = -left\n",
    "        forward_flipped = forward\n",
    "        return image_flip, forward_flipped, left_flipped\n",
    "\n",
    "    data_loader = load_data(paths)\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for image, forward, left in data_loader:\n",
    "\n",
    "        image = preprocess(image, image_size)\n",
    "        \n",
    "        image_flipped, forward_flipped, left_flipped = augment(image, forward, left)\n",
    "        \n",
    "        image = img_to_array(image).astype(np.float32) / 255.0\n",
    "        image_flipped = img_to_array(image_flipped).astype(np.float32) / 255.0\n",
    "        \n",
    "        batch_images.append(image_flipped)\n",
    "        batch_labels.append([forward_flipped, left_flipped])\n",
    "    \n",
    "        batch_images.append(image)\n",
    "        batch_labels.append([forward, left])\n",
    "\n",
    "        if len(batch_images) == 2 * batch_size:\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            \n",
    "    if batch_images:\n",
    "        yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d8e41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_wrapper(paths):\n",
    "    for batch_images, batch_labels in load_data_preprocess(paths, 64, 32):\n",
    "        yield batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "43318ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = glob(\"../dataset/*.csv\")\n",
    "random.shuffle(csv_paths)\n",
    "\n",
    "\n",
    "test_paths = csv_paths[:3]\n",
    "train_val_paths = csv_paths[3:]\n",
    "\n",
    "val_path = train_val_paths[0]        \n",
    "train_paths = train_val_paths[1:] \n",
    "\n",
    "img_size = 64\n",
    "batch_size = 16\n",
    "\n",
    "test_loader = load_data_preprocess(test_paths, img_size, batch_size)\n",
    "train_loader = load_data_preprocess(train_paths, img_size, batch_size)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generator_wrapper(train_paths),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 64, 64, 1], [None, 2])\n",
    ").repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generator_wrapper([val_path]),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 64, 64, 1], [None, 2])\n",
    ").repeat()\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generator_wrapper(test_paths),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 64, 64, 1], [None, 2])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "12856913",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_samples = sum(\n",
    "    [sum(1 for _ in load_data_preprocess([path], 64, batch_size)) * batch_size * 2 for path in train_paths]\n",
    ")\n",
    "steps_per_epoch = total_train_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5d99e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_val_samples = sum(\n",
    "    [sum(1 for _ in load_data_preprocess([path], 64, batch_size)) * batch_size * 2 for path in [val_path]]\n",
    ")\n",
    "val_steps = total_val_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2de78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(forward_weight=3.0, left_weight=7.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        forward_true = y_true[:, 0]\n",
    "        left_true = y_true[:, 1]\n",
    "\n",
    "        forward_pred = y_pred[:, 0]\n",
    "        left_pred = y_pred[:, 1]\n",
    "\n",
    "        forward_mse = tf.reduce_mean(tf.square(forward_true - forward_pred))\n",
    "        left_mse = tf.reduce_mean(tf.square(left_true - left_pred))\n",
    "\n",
    "        return forward_weight * forward_mse + left_weight * left_mse\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9da9987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(image_size):\n",
    "    inputs = Input(shape=(image_size, image_size, 1))\n",
    "    x = Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    outputs = Dense(2, activation=\"tanh\")(x)\n",
    "\n",
    "    return Model(inputs = inputs, outputs = outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4f0966e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_forward(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true[:, 0] - y_pred[:, 0]))\n",
    "\n",
    "def mse_left(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true[:, 1] - y_pred[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "065ef75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 64, 64, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 64, 64, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 64, 64, 16)        64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 32, 32, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 32, 32, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 16, 16, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                524352    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 531490 (2.03 MB)\n",
      "Trainable params: 531394 (2.03 MB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(image_size=img_size)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "10ada5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=weighted_mse_loss(forward_weight=2.0, left_weight=3.0),\n",
    "    metrics=['mse', mse_forward, mse_left],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0948c94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "590/590 [==============================] - 224s 376ms/step - loss: 0.4154 - mse: 0.0751 - mse_forward: 0.0355 - mse_left: 0.1146 - val_loss: 0.2182 - val_mse: 0.0373 - val_mse_forward: 0.0056 - val_mse_left: 0.0687\n",
      "Epoch 2/20\n",
      "590/590 [==============================] - 199s 338ms/step - loss: 0.1521 - mse: 0.0312 - mse_forward: 0.0353 - mse_left: 0.0272 - val_loss: 0.2006 - val_mse: 0.0339 - val_mse_forward: 0.0031 - val_mse_left: 0.0648\n",
      "Epoch 3/20\n",
      "590/590 [==============================] - 222s 377ms/step - loss: 0.1369 - mse: 0.0287 - mse_forward: 0.0354 - mse_left: 0.0220 - val_loss: 0.2757 - val_mse: 0.0465 - val_mse_forward: 0.0031 - val_mse_left: 0.0901\n",
      "Epoch 4/20\n",
      "590/590 [==============================] - 216s 366ms/step - loss: 0.1218 - mse: 0.0261 - mse_forward: 0.0352 - mse_left: 0.0171 - val_loss: 0.2573 - val_mse: 0.0434 - val_mse_forward: 0.0031 - val_mse_left: 0.0839\n",
      "Epoch 5/20\n",
      "590/590 [==============================] - 215s 365ms/step - loss: 0.1115 - mse: 0.0244 - mse_forward: 0.0350 - mse_left: 0.0139 - val_loss: 0.2461 - val_mse: 0.0415 - val_mse_forward: 0.0030 - val_mse_left: 0.0800\n",
      "Epoch 6/20\n",
      "590/590 [==============================] - 198s 335ms/step - loss: 0.1003 - mse: 0.0219 - mse_forward: 0.0309 - mse_left: 0.0128 - val_loss: 0.3783 - val_mse: 0.0681 - val_mse_forward: 0.0303 - val_mse_left: 0.1061\n",
      "Epoch 7/20\n",
      "590/590 [==============================] - 200s 339ms/step - loss: 0.0814 - mse: 0.0171 - mse_forward: 0.0211 - mse_left: 0.0131 - val_loss: 0.3400 - val_mse: 0.0582 - val_mse_forward: 0.0090 - val_mse_left: 0.1071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f243e3fb9d0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    \n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "81c58ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 22:13:29.194261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.194336: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2025-05-17 22:13:29.194618: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2025-05-17 22:13:29.195399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.195515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.195594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.195962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.195975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-05-17 22:13:29.196055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.196085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-05-17 22:13:29.297261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.297418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.297483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.297857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.297893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-05-17 22:13:29.297983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.298017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-05-17 22:13:29.312853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.312914: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2025-05-17 22:13:29.313029: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2025-05-17 22:13:29.313394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.313488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.313563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.313828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.313841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-05-17 22:13:29.313918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-17 22:13:29.313970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "spec = (tf.TensorSpec((None, 64, 64, 1), tf.float32, name=\"input\"),)\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0ae92431",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_opset11.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "333b0f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 50s 382ms/step - loss: 0.2682 - mse: 0.0536 - mse_forward: 0.0531 - mse_left: 0.0539\n",
      "Test Results:\n",
      "loss: 0.2682\n",
      "mse: 0.0536\n",
      "mse_forward: 0.0531\n",
      "mse_left: 0.0539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 22:16:40.047972: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1095418138162098337\n",
      "2025-05-17 22:16:40.048030: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11859574868892307873\n",
      "2025-05-17 22:16:40.048054: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15115606165445385997\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_loader, return_dict=True)\n",
    "\n",
    "print(\"Test Results:\")\n",
    "for name, value in results.items():\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "44dce5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 28ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 48ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 28ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 32ms/step\n",
      "2/2 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 32ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 29ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 33ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 22ms/step\n",
      "2/2 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 29ms/step\n",
      "2/2 [==============================] - 0s 22ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 22ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Total MSE: 0.0536\n",
      "MSE Forward: 0.0531\n",
      "MSE Left: 0.0540\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Collect the true labels and predictions\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "for batch_x, batch_y in test_dataset.take(100):  # or however many you want to test\n",
    "    preds = model.predict(batch_x)\n",
    "    y_pred_list.append(preds)\n",
    "    y_true_list.append(batch_y.numpy())\n",
    "\n",
    "# Convert lists to arrays\n",
    "y_pred_all = np.vstack(y_pred_list)\n",
    "y_true_all = np.vstack(y_true_list)\n",
    "\n",
    "# Compute total MSE\n",
    "total_mse = mean_squared_error(y_true_all, y_pred_all)\n",
    "\n",
    "# Compute per-output MSE\n",
    "mse_forward = mean_squared_error(y_true_all[:, 0], y_pred_all[:, 0])\n",
    "mse_left = mean_squared_error(y_true_all[:, 1], y_pred_all[:, 1])\n",
    "\n",
    "print(f\"Total MSE: {total_mse:.4f}\")\n",
    "print(f\"MSE Forward: {mse_forward:.4f}\")\n",
    "print(f\"MSE Left: {mse_left:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
